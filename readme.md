# ğŸ“˜ Supervised Learning Algorithms  

---
A comprehensive repository that covers all major Supervised Machine Learning algorithms ğŸ“˜ â€” explained in detail with theory, Python code, ASCII flowcharts, and practical implementation.

To make it even more valuable, I implemented every model on the Titanic Dataset ğŸ›³ï¸ (Survival Prediction) â€” one of the most popular datasets in Data Science.


## ğŸ“‘ Table of Contents  
1. [Overview of Supervised Learning](#-overview-of-supervised-learning)  
2. [Algorithms](#-algorithms)  
   - [Linear Regression](#1-linear-regression)  
   - [Logistic Regression](#2-logistic-regression)  
   - [K-Nearest Neighbors (KNN)](#3-k-nearest-neighbors-knn)  
   - [Decision Tree](#4-decision-tree)  
   - [Random Forest](#5-random-forest)  
   - [Support Vector Machine (SVM)](#6-support-vector-machine-svm)  
   - [NaÃ¯ve Bayes](#7-naÃ¯ve-bayes)  
   - [Gradient Boosting](#8-gradient-boosting-xgboost-lightgbm-catboost)  
   - [Neural Networks (Intro)](#9-neural-networks-intro)  
3. [Comparison Table](#-comparison-table)  
4. [Choosing the Right Algorithm (Flowchart)](#-choosing-the-right-algorithm-ascii-flowchart)  
5. [Installation](#-installation)  
6. [Cheatsheet (Quick Revision)](#-cheatsheet-quick-revision)  
7. [References](#-references)  
8. [Credits](#-credits)  

---

## ğŸ”¹ Overview of Supervised Learning  
- **Definition**: Algorithms that learn from **labeled data** (input `X` â†’ output `Y`).  
- **Goal**: Map input features â†’ output target.  
- **Types**:  
  - **Regression** â†’ Predict continuous values (e.g., house price).  
  - **Classification** â†’ Predict categorical values (e.g., spam or not spam).  
- **Workflow**:  
```

Data â†’ Preprocess â†’ Train â†’ Test â†’ Evaluate â†’ Deploy

````

---

## ğŸ”¹ Algorithms  

### 1. Linear Regression  
- **Type**: Regression  
- **Mathematical Idea**:  
- Predicts continuous target using a **linear equation**.  
- Equation: `y = b0 + b1x1 + b2x2 + ... + bn*xn`  
- **When to Use**:  
- Relationship between features & target is linear.  
- **Pros**: Simple, interpretable.  
- **Cons**: Sensitive to outliers.  
- **Code**:  
```python
from sklearn.linear_model import LinearRegression
model = LinearRegression().fit(X_train, y_train)
pred = model.predict(X_test)
````

* **Flowchart**:

  ```
  Input Data
     â†“
  Fit Line y = mx + c
     â†“
  Minimize Squared Error
     â†“
  Predict Continuous Value
  ```

---

### 2. Logistic Regression

* **Type**: Classification
* **Mathematical Idea**:

  * Uses **sigmoid function** for probability.
* **When to Use**: Binary classification.
* **Pros**: Simple, probabilistic.
* **Cons**: Poor for non-linear data.
* **Code**:

  ```python
  from sklearn.linear_model import LogisticRegression
  model = LogisticRegression().fit(X_train, y_train)
  pred = model.predict(X_test)
  ```
* **Flowchart**:

  ```
  Input Features
      â†“
  Linear Combination (z = b0 + b1x1 + ...)
      â†“
  Apply Sigmoid Ïƒ(z)
      â†“
  Probability (0-1)
      â†“
  Threshold â†’ Class Label
  ```

---

### 3. K-Nearest Neighbors (KNN)

* **Type**: Both
* **Idea**: Classifies based on nearest `k` neighbors.
* **When to Use**: Small datasets, pattern recognition.
* **Pros**: No training.
* **Cons**: Slow on large datasets.
* **Code**:

  ```python
  from sklearn.neighbors import KNeighborsClassifier
  model = KNeighborsClassifier(n_neighbors=5).fit(X_train, y_train)
  pred = model.predict(X_test)
  ```
* **Flowchart**:

  ```
  Input Data Point
        â†“
  Compute Distance to All Points
        â†“
  Select k Nearest Neighbors
        â†“
  Majority Vote (Classification)
  or Average (Regression)
  ```

---

### 4. Decision Tree

* **Type**: Both
* **Idea**: Splits data using rules â†’ tree.
* **Pros**: Easy to interpret.
* **Cons**: Overfits.
* **Code**:

  ```python
  from sklearn.tree import DecisionTreeClassifier
  model = DecisionTreeClassifier().fit(X_train, y_train)
  pred = model.predict(X_test)
  ```
* **Flowchart**:

  ```
  Start
    â†“
  Choose Best Feature (Gini/Entropy)
    â†“
  Split Data into Subsets
    â†“
  Repeat Until Leaf Node
    â†“
  Leaf â†’ Prediction
  ```

---

### 5. Random Forest

* **Type**: Both
* **Idea**: Multiple trees â†’ voting/averaging.
* **Pros**: Robust, less overfit.
* **Cons**: Black-box.
* **Code**:

  ```python
  from sklearn.ensemble import RandomForestClassifier
  model = RandomForestClassifier().fit(X_train, y_train)
  pred = model.predict(X_test)
  ```
* **Flowchart**:

  ```
  Input Data
     â†“
  Bootstrap Samples
     â†“
  Build Many Decision Trees
     â†“
  Aggregate Predictions
     â†“
  Final Output
  ```

---

### 6. Support Vector Machine (SVM)

* **Type**: Both
* **Idea**: Finds best hyperplane with max margin.
* **Pros**: Works well in high dimensions.
* **Cons**: Slow for large data.
* **Code**:

  ```python
  from sklearn.svm import SVC
  model = SVC(kernel='rbf').fit(X_train, y_train)
  pred = model.predict(X_test)
  ```
* **Flowchart**:

  ```
  Input Data
     â†“
  Map Data (Kernel Trick)
     â†“
  Find Optimal Hyperplane
     â†“
  Maximize Margin Between Classes
     â†“
  Predict Class
  ```

---

### 7. NaÃ¯ve Bayes

* **Type**: Classification
* **Idea**: Uses **Bayesâ€™ theorem** + independence assumption.
* **Pros**: Fast, works with text.
* **Cons**: Assumes independence.
* **Code**:

  ```python
  from sklearn.naive_bayes import GaussianNB
  model = GaussianNB().fit(X_train, y_train)
  pred = model.predict(X_test)
  ```
* **Flowchart**:

  ```
  Input Features
     â†“
  Calculate Prior Probability
     â†“
  Calculate Likelihood for Each Class
     â†“
  Apply Bayesâ€™ Rule
     â†“
  Choose Class with Max Probability
  ```

---

### 8. Gradient Boosting (XGBoost, LightGBM, CatBoost)

* **Type**: Both
* **Idea**: Sequentially builds trees â†’ corrects errors.
* **Pros**: High accuracy.
* **Cons**: Complex, tuning required.
* **Code**:

  ```python
  from xgboost import XGBClassifier
  model = XGBClassifier().fit(X_train, y_train)
  pred = model.predict(X_test)
  ```
* **Flowchart**:

  ```
  Start with Weak Model
        â†“
  Compute Residual Errors
        â†“
  Train New Tree on Errors
        â†“
  Add Tree to Ensemble
        â†“
  Repeat â†’ Final Strong Model
  ```

---

### 9. Neural Networks (Intro)

* **Type**: Both
* **Idea**: Layers of neurons â†’ non-linear transformations.
* **Pros**: Very powerful.
* **Cons**: Needs huge data & compute.
* **Code**:

  ```python
  from sklearn.neural_network import MLPClassifier
  model = MLPClassifier(hidden_layer_sizes=(100,), max_iter=500).fit(X_train, y_train)
  pred = model.predict(X_test)
  ```
* **Flowchart**:

  ```
  Input Layer (Features)
        â†“
  Hidden Layers (Weights + Activation)
        â†“
  Backpropagation (Error Correction)
        â†“
  Output Layer (Prediction)
  ```

---

## ğŸ“Š Comparison Table

| Algorithm           | Type           | Pros                    | Cons                       |
| ------------------- | -------------- | ----------------------- | -------------------------- |
| Linear Regression   | Regression     | Simple, interpretable   | Only linear data           |
| Logistic Regression | Classification | Fast, interpretable     | Poor for non-linear data   |
| KNN                 | Both           | Simple, no training     | Slow, scaling needed       |
| Decision Tree       | Both           | Easy visualization      | Overfits                   |
| Random Forest       | Both           | Accurate, stable        | Slower, less interpretable |
| SVM                 | Both           | Works in high-dim space | Slow for big datasets      |
| NaÃ¯ve Bayes         | Classification | Fast, good for text     | Assumes independence       |
| Gradient Boosting   | Both           | Very accurate           | Complex, tuning needed     |
| Neural Networks     | Both           | Powerful, flexible      | Data & compute hungry      |

---

## ğŸ¯ Choosing the Right Algorithm (ASCII Flowchart)

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚ How big is your dataset?â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚                               â”‚
        Small Dataset                    Large Dataset
                â”‚                               â”‚
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚                          â”‚       â”‚                  â”‚
 Linear / Logistic       Non-linear?   Need high accuracy?
 Regression                   â”‚             â”‚
                              â”‚             â”‚
                        â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â”‚   Yes     â”‚  â”‚       Yes        â”‚
                        â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚                â”‚
                         KNN / Decision   Random Forest /
                             Tree         Gradient Boosting
                                              â”‚
                                    Deep patterns? (images/text)
                                              â”‚
                                         Neural Networks
```

---

## âš™ï¸ Installation

```bash
pip install numpy pandas scikit-learn xgboost lightgbm catboost
```

---

## ğŸ“„ Cheatsheet (Quick Revision)

* **Linear Regression** â†’ `y = b0 + b1x` â†’ Continuous output
* **Logistic Regression** â†’ `Ïƒ(z) = 1 / (1 + e^-z)` â†’ Binary classification
* **KNN** â†’ Look at `k` closest neighbors â†’ vote/average
* **Decision Tree** â†’ Splits dataset by features â†’ if/else rules
* **Random Forest** â†’ Multiple trees â†’ majority vote â†’ stable
* **SVM** â†’ Max-margin hyperplane â†’ kernel trick
* **NaÃ¯ve Bayes** â†’ Bayes theorem + independence assumption
* **Gradient Boosting** â†’ Sequential trees fixing errors â†’ very accurate
* **Neural Networks** â†’ Layers + activations + backpropagation â†’ powerful

---

## ğŸ“š References

* [Scikit-Learn Documentation](https://scikit-learn.org/stable/)
* [XGBoost Documentation](https://xgboost.readthedocs.io/)
* [Deep Learning Book (Ian Goodfellow)](https://www.deeplearningbook.org/)

---

## ğŸ™Œ Credits

* Created by **Himanshu Kumar**

